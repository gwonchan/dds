
###1.	How many breweries are present in each state



```{r}

hello_breweries = read.csv("Breweries.csv")
hello_beers = read.csv("Beers.csv")
dim(hello_breweries)
head(hello_breweries)
dim(hello_beers)
head(hello_beers)

#by states
hello1=table(hello_breweries$State)
hello1
dim(hello1)
plot(hello1,main = "Breweries by state",xlab = "States", ylab = "# of Breweries",ylim=c(0,60),cex.axis = 1, las=2)
```
##2.	Merge beer data with the breweries data. Print the first 6 observations and the last six observations to check the merged file.  (RMD only, this does not need to be included in the presentation or the deck.)

```{r}
hello_merge = merge(hello_breweries,hello_beers,by.x = "Brew_ID", by.y = "Brewery_id")
dim(hello_merge)
head(hello_merge)
filter(hello_breweries, "Brew_ID" == "1")
filter(hello_beers, "Beer_ID" == "2689")
tail(hello_merge)
filter(hello_breweries, "Brew_ID" == "558")
filter(hello_beers, "Beer_ID" == "30")

```
##3.	Address the missing values in each column.
```{r}
hello_NA = hello_merge[!complete.cases(hello_merge),]
dim(hello_NA)
head(hello_NA)
hello_NNA = hello_merge[complete.cases(hello_merge),]
dim(hello_NNA)
head(hello_NNA)
library(naniar)
gg_miss_var(hello_merge)
```
##4.	Compute the median alcohol content and international bitterness unit for each state. Plot a bar chart to compare.
```{r}
library(dplyr)
dplyr::summarize
hello1 <- hello_merge %>% group_by(State) %>% summarize(medianABV = median(ABV,na.rm = TRUE), count = n()) %>% arrange(medianABV)
print(hello1)
barplot(hello1$medianABV, names.arg = hello1$State, las=2)

hello2 <- hello_merge %>% group_by(State) %>% summarize(medianIBU = median(IBU,na.rm = TRUE), count = n()) %>% arrange(medianIBU)
print(hello2)
barplot(hello2$medianIBU, names.arg = hello2$State, las=2)
```

##5.	Which state has the maximum alcoholic (ABV) beer? Which state has the most bitter (IBU) beer?
```{r}
hello_ABV <- hello_merge %>% group_by(State)%>%summarize(max_alc=max(ABV,na.rm=TRUE)) %>% arrange(desc(max_alc))
hello_IBU <- hello_merge %>% group_by(State)%>%summarize(max_ibu=max(IBU,na.rm=TRUE)) %>% arrange(max_ibu)
hello_ABV
hello_IBU
```
6.	Comment on the summary statistics and distribution of the ABV variable.
```{r}
summary(hello_merge$ABV)
hist(hello_merge$ABV,main = "Histogram of ABV",xlab = "ABV", ylab = "Count")
```
7.	Is there an apparent relationship between the bitterness of the beer and its alcoholic content? Draw a scatter plot.  Make your best judgment of a relationship and EXPLAIN your answer.
```{r}
library(ggplot2)
hello_merge %>% ggplot(aes(x = ABV, y = IBU)) + 
geom_point(color = "blue")
scatter.smooth(x=hello_merge$ABV, y=hello_merge$IBU, main = "Mild positive linear relationship",xlab = "ABV", ylab = "IBU", col="blue")
print("This data shows mild positive linear relationship between bitterness and alcoholic content(3rd degree polynomial). Most of Beer bitterness comes from hops. ")
```
8.	Budweiser would also like to investigate the difference with respect to IBU and ABV between IPAs (India Pale Ales) and other types of Ale (any beer with “Ale” in its name other than IPA).  You decide to use KNN classification to investigate this relationship.  Provide statistical evidence one way or the other. You can of course assume your audience is comfortable with percentages … KNN is very easy to understand conceptually. 
In addition, while you have decided to use KNN to investigate this relationship (KNN is required) you may also feel free to supplement your response to this question with any other methods or techniques you have learned.  Creativity and alternative solutions are always encouraged.  
```{r}
library(XML) #xml_Parse
library(dplyr)
library(tidyr)
library(stringi)
library(rvest) #html_table, html_node
library(ggplot2)
library(RCurl) #getURL
library(class)
library(caret)
library(e1071)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("stringr")     # Install & load stringr package
library("stringr")
library(naniar)

#classfy IPAs or not, using IBU and ABV values.
#make new column as IPAs and the values will be 1 or 0. When the Style has "IPA", the value will be 1. nor 0.
hello_IPA <- 1*str_detect(hello_merge$Style, "IPA")
hello_IPA
hello_merge$IPAs <- hello_IPA
head(hello_merge)

hello_merge %>% ggplot(aes(x = ABV, y = IBU, color=IPAs > 0)) + 
geom_point() +
stat_ellipse()

#median IPA
hello_filter1=filter(hello_merge, IPAs == "1")
median(hello_filter1$ABV[which(!is.na(hello_filter1$ABV))])
median(hello_filter1$IBU[which(!is.na(hello_filter1$IBU))])

#median no IPA
hello_filter0=filter(hello_merge, IPAs == "0")
median(hello_filter0$ABV[which(!is.na(hello_filter0$ABV))])
median(hello_filter0$IBU[which(!is.na(hello_filter0$IBU))])


#impute missing values with median
hello_impute <- hello_merge
hello_impute$IBU <- impute(hello_impute$IBU, median)
hello_impute$ABV <- impute(hello_impute$ABV, median)
hello_impute = hello_impute[complete.cases(hello_impute),]
gg_miss_var(hello_impute)

#scaling
hello_scale = hello_impute
hello_scale$IBU = scale(hello_impute$IBU)
hello_scale$ABV = scale(hello_impute$ABV)
hello_scale %>% ggplot(aes(x = ABV, y = IBU, color=IPAs > 0)) + 
geom_point() +
stat_ellipse()

#could not find function "impute.knn"
#install.packages("imputeTS")
#library(imputeTS)
#impute.knn(hello_merge ,k = 10, rowmax = 0.5, colmax = 0.8, maxp = 1500, rng.seed=362436069)
```

```{r}
#70/30 training/
# Loop for many k and the average of many training / test partition

set.seed(1)
iterations = 10
numks = 20
splitPerc = .70

masterAcc = matrix(nrow = iterations, ncol = numks)


for(j in 1:iterations)
{
  trainIndices = sample(1:dim(hello_scale)[1],round(splitPerc * dim(hello_scale)[1]))
  train = hello_scale[trainIndices,]
  test = hello_scale[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(train[,c(7,8)],test[,c(7,8)],train$IPAs, prob = TRUE, k = i)
    table(classifications,test$IPAs)
    CM = confusionMatrix(table(classifications,test$IPAs))
    masterAcc[j,i] = CM$overall[1]
  }
  
}

MeanAcc = colMeans(masterAcc)

plot(seq(1,numks,1),MeanAcc, type = "l")

which.max(MeanAcc)
max(MeanAcc)
```

```{r}
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 20.
classifications = knn(train[,c(7,8)],test[,c(7,8)],train$IPAs, prob = TRUE, k = 5)
table(classifications,test$IPAs)
CM = confusionMatrix(table(classifications,test$IPAs))
```
```{r}
#decision boundary
prob1 <- attr(classifications, "prob")
px1 <- test$ABV
px2 <- test$IBU


mat = matrix(, nrow = 723, ncol = 3)
mat[,1] <- px1
mat[,2] <- px2
mat[,3] <- classifications

dff <- data.frame(mat)
colnames(dff) <- c('var1', 'var2', 'var3')

ggplot(dff)+geom_point(aes(x = var1, y = var2, col=var3))+
geom_contour(aes(x = var1, y = var2, z = var3))
```

matt = filter(mat, mat[,3] == "1")

px11 <- px1[1:10]
px22 <- px2[1:10]


pgrid1 <- expand.grid(px11=px11, px22=px22)
pgrid1$prob <- prob1[1:10]
matrix_val1 <- matrix(pgrid1$prob, 
                     length(px11), 
                     length(px22))
contour(px11, px22, matrix_val1, levels=1, labels="", xlab="", ylab="", main=
          "Some Picture", lwd=2, axes=FALSE)
          
library(plotly)       
fig1 <- plot_ly(
px1,px2,matrix_val1,type = "contour")
          
          
          
library(plotly)
fig <- plot_ly(
  x = c(-9, -6, -5, -3, -1), 
  y = c(0, 1, 4, 5, 7), 
  z = matrix(c(10, 10.625, 12.5, 15.625, 20, 5.625, 6.25, 8.125, 11.25, 15.625, 2.5, 3.125, 5, 8.125, 12.5, 0.625, 1.25, 3.125,
        6.25, 10.625, 0, 0.625, 2.5, 5.625, 10), nrow = 5, ncol = 5), 
  type = "contour" 
)

fig

#plot when prob1 = 1

prob15 <- as.matrix(prob1,length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob15, levels=0.5, labels="", xlab="", ylab="", main=
        "15-nearest neighbour", axes=FALSE)
```
##example

x1 <- 1:200
x2 <- 50:250

cgrid <- expand.grid(x1=x1, x2=x2)
# our "prediction" colours the bottom left quadrant
cgrid$prob <- 1
cgrid[cgrid$x1 < 100 & cgrid$x2 < 170, c("prob")] <- 0

matrix_val <- matrix(cgrid$prob, 
                     length(x1), 
                     length(x2))

contour(x1, x2, matrix_val, levels=0.5, labels="", xlab="", ylab="", main=
          "Some Picture", lwd=2, axes=FALSE)
gd <- expand.grid(x=x1, y=x2)
points(gd, pch=".", cex=1.2, col=ifelse(prob==1, "coral", "cornflowerblue"))
box()

##9.	Knock their socks off!  Find one other useful inference from the data that you feel Budweiser may be able to find value in.  You must convince them why it is important and back up your conviction with appropriate statistical evidence. 
```{r}
##market share, beers by breweries
freq_table=table(hello_merge$Name.x)
plot(freq_table,main = "Breweries by competitors",xlab = "Names", ylab = "# of Breweries",ylim=c(0,50),cex.axis = 1, las=2)
#top three
hello_market <- hello_merge %>% group_by(Name.x)%>%tally(sort = TRUE)
hello_market

```